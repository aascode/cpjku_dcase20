{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,json,os,importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the pretrained models from https://zenodo.org/record/4282667 and extract the models directories into the \"pretrained_models\" directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sub1',\n",
       " 'README.md',\n",
       " 'sub3',\n",
       " 'ensem2',\n",
       " '.gitignore',\n",
       " 'sub2',\n",
       " 'ensem3',\n",
       " 'ensem1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"pretrained_models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the pretrained model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path=\"pretrained_models/sub1/\"\n",
    "model_path=\"pretrained_models/sub2/\"\n",
    "#model_path=\"pretrained_models/sub3/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model specified by model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset_config.json',\n",
       " 'eval_predictions_probs.pth',\n",
       " 'description.txt',\n",
       " 'eval_predictions.csv',\n",
       " 'model_from_training_state_dict.pth',\n",
       " 'model_config.json',\n",
       " 'model_state_dict.pth']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model config from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path+'model_config.json',\"r\")as f:\n",
    "    model_config=json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making an instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n_blocks_per_stage is specified ignoring the depth param, nc=None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning Pruning only 1x1 CONVS\n",
      "\n",
      "WARNING: stage3 removed\n",
      "0 0.05892556509887896 0.408248290463863\n",
      "1 0.05892556509887896 0.408248290463863\n",
      "2 0.05892556509887896 0.408248290463863\n",
      "3 0.1767766952966369 0.408248290463863\n",
      "4 0.125 0.408248290463863\n",
      "5 0.125 0.408248290463863\n"
     ]
    }
   ],
   "source": [
    "module = importlib.import_module('models.{}'.format(model_config['arch']))\n",
    "Network = getattr(module, 'Network')\n",
    "model=Network(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict=torch.load(model_path+\"/model_state_dict.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model in float16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if the weights are float16 cast the model\n",
    "if [v for k,v in state_dict.items()][0].dtype==torch.float16:\n",
    "    model=model.half()\n",
    "    print(\"model in float16\")\n",
    "model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_non_zero_params_nonbn(model):\n",
    "    sum_params = 0\n",
    "    sum_non_zero= 0 \n",
    "    desc=\"\"\n",
    "    def calc_params(model):\n",
    "        nonlocal desc, sum_params, sum_non_zero\n",
    "        skip=\"count\"\n",
    "        if \"batchnorm\" in  type(model).__name__.lower():\n",
    "            skip=\"skip\"\n",
    "        for k,p in model.named_parameters(recurse=False):\n",
    "            if p.requires_grad:\n",
    "                nonzero = (p != 0).sum()\n",
    "                total= p.numel()\n",
    "                desc += f\"type {type(model).__name__}, {k},  {total}, {nonzero}, {p.dtype}, {skip} \" + \"\\n\"\n",
    "                if skip!=\"skip\":\n",
    "                    sum_params+=total\n",
    "                    sum_non_zero+=nonzero\n",
    "    model.apply(calc_params)       \n",
    "    return desc,sum_params,sum_non_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:  343552 Non-Zero: tensor(247562, device='cuda:0')\n",
      "type Conv2d, weight,  3200, 3200, torch.float16, count \n",
      "type BatchNorm2d, weight,  64, 64, torch.float16, skip \n",
      "type BatchNorm2d, bias,  64, 64, torch.float16, skip \n",
      "type Conv2dDamped, weight,  36864, 36864, torch.float16, count \n",
      "type BatchNorm2d, weight,  64, 64, torch.float16, skip \n",
      "type BatchNorm2d, bias,  64, 64, torch.float16, skip \n",
      "type Conv2dPrune, weight,  4096, 2698, torch.float16, count \n",
      "type Conv2dPrune, prune_mask,  4096, 0, torch.float16, count \n",
      "type BatchNorm2d, weight,  64, 64, torch.float16, skip \n",
      "type BatchNorm2d, bias,  64, 64, torch.float16, skip \n",
      "type Conv2dDamped, weight,  36864, 36864, torch.float16, count \n",
      "type BatchNorm2d, weight,  64, 64, torch.float16, skip \n",
      "type BatchNorm2d, bias,  64, 64, torch.float16, skip \n",
      "type Conv2dDamped, weight,  36864, 36864, torch.float16, count \n",
      "type BatchNorm2d, weight,  64, 64, torch.float16, skip \n",
      "type BatchNorm2d, bias,  64, 64, torch.float16, skip \n",
      "type Conv2dDamped, weight,  36864, 36864, torch.float16, count \n",
      "type BatchNorm2d, weight,  64, 64, torch.float16, skip \n",
      "type BatchNorm2d, bias,  64, 64, torch.float16, skip \n",
      "type Conv2dDamped, weight,  36864, 36864, torch.float16, count \n",
      "type BatchNorm2d, weight,  64, 64, torch.float16, skip \n",
      "type BatchNorm2d, bias,  64, 64, torch.float16, skip \n",
      "type Conv2dPrune, weight,  4096, 3438, torch.float16, count \n",
      "type Conv2dPrune, prune_mask,  4096, 0, torch.float16, count \n",
      "type BatchNorm2d, weight,  64, 64, torch.float16, skip \n",
      "type BatchNorm2d, bias,  64, 64, torch.float16, skip \n",
      "type Conv2dPrune, weight,  4096, 2985, torch.float16, count \n",
      "type Conv2dPrune, prune_mask,  4096, 0, torch.float16, count \n",
      "type BatchNorm2d, weight,  64, 64, torch.float16, skip \n",
      "type BatchNorm2d, bias,  64, 64, torch.float16, skip \n",
      "type Conv2dPrune, weight,  8192, 6731, torch.float16, count \n",
      "type Conv2dPrune, prune_mask,  8192, 0, torch.float16, count \n",
      "type BatchNorm2d, weight,  128, 128, torch.float16, skip \n",
      "type BatchNorm2d, bias,  128, 128, torch.float16, skip \n",
      "type Conv2dPrune, weight,  16384, 11589, torch.float16, count \n",
      "type Conv2dPrune, prune_mask,  16384, 0, torch.float16, count \n",
      "type BatchNorm2d, weight,  128, 128, torch.float16, skip \n",
      "type BatchNorm2d, bias,  128, 128, torch.float16, skip \n",
      "type Conv2dPrune, weight,  8192, 6981, torch.float16, count \n",
      "type Conv2dPrune, prune_mask,  8192, 0, torch.float16, count \n",
      "type BatchNorm2d, weight,  128, 128, torch.float16, skip \n",
      "type BatchNorm2d, bias,  128, 128, torch.float16, skip \n",
      "type Conv2dPrune, weight,  16384, 13910, torch.float16, count \n",
      "type Conv2dPrune, prune_mask,  16384, 0, torch.float16, count \n",
      "type BatchNorm2d, weight,  128, 128, torch.float16, skip \n",
      "type BatchNorm2d, bias,  128, 128, torch.float16, skip \n",
      "type Conv2dPrune, weight,  16384, 11326, torch.float16, count \n",
      "type Conv2dPrune, prune_mask,  16384, 0, torch.float16, count \n",
      "type BatchNorm2d, weight,  128, 128, torch.float16, skip \n",
      "type BatchNorm2d, bias,  128, 128, torch.float16, skip \n",
      "type Conv2d, weight,  384, 384, torch.float16, count \n",
      "type BatchNorm2d, weight,  3, 3, torch.float16, skip \n",
      "type BatchNorm2d, bias,  3, 3, torch.float16, skip \n",
      "\n"
     ]
    }
   ],
   "source": [
    "desc,sum_params,sum_non_zero=count_non_zero_params_nonbn(model)\n",
    "print(\"Total: \",sum_params, \"Non-Zero:\",sum_non_zero)\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "# the eval json config dataset specifies that the norms should be calculated from all the training clips. \n",
    "with open('configs/datasets/dcase2020b_eval.json',\"r\")as f:\n",
    "    dataset_config=json.load(f)\n",
    "datasetm=datasets.DatasetsManager(dataset_config['audiodataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset from 'd20t1b'\n",
      "normalizing dataset\n",
      "normalized train!\n",
      "attempting to load x from cache at datasets/cached_datasets/d20t1b/tr_mean_676d70_f1_apd18_stereo_obj.pt...\n",
      "loaded datasets/cached_datasets/d20t1b/tr_mean_676d70_f1_apd18_stereo_obj.pt from cache in 0.0005588531494140625 \n",
      "attempting to load x from cache at datasets/cached_datasets/d20t1b/tr_std_676d70_f1_apd18_stereo_obj.pt...\n",
      "loaded datasets/cached_datasets/d20t1b/tr_std_676d70_f1_apd18_stereo_obj.pt from cache in 0.0005135536193847656 \n",
      "normalized train!\n",
      "normalized test!\n",
      "normalized SUB dataset!\n",
      "loading dataset from 'd20t1b_sub'\n"
     ]
    }
   ],
   "source": [
    "trainds=datasetm.get_train_set()\n",
    "testds=datasetm.get_train_set()\n",
    "evalds=datasetm.get_sub_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dev train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([1, 2, 256, 431])\n",
      "in_c: torch.Size([1, 64, 127, 215])\n",
      "stage1: torch.Size([1, 64, 15, 26])\n",
      "stage2: torch.Size([1, 128, 15, 26])\n",
      "stage3: torch.Size([1, 128, 15, 26])\n",
      "feed_forward: torch.Size([1, 3, 1, 1])\n",
      "logit: torch.Size([1, 3])\n",
      "all  14400  correct  14400  ratio= 1.0\n"
     ]
    }
   ],
   "source": [
    "all_y=0\n",
    "corr_y=0\n",
    "model.eval()\n",
    "model.cuda()\n",
    "half_mode=True\n",
    "data_loader = torch.utils.data.DataLoader(trainds,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=False)\n",
    "with torch.no_grad():\n",
    "    for i,(x,idx,y) in enumerate(data_loader):\n",
    "        if i%10==0:\n",
    "            print(i,\"/\",len(data_loader),end=\"\\r\")\n",
    "        x=x.cuda()\n",
    "        if half_mode:\n",
    "            x=x.half()\n",
    "        out=model(x)\n",
    "        all_y+=1\n",
    "        corr_y+=(torch.argmax(out).cpu()==y).sum().item()\n",
    "print(\"all \", all_y,\" correct \",corr_y,\" ratio=\",corr_y*1./all_y) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all  8640  correct  0  ratio= 0.0\n"
     ]
    }
   ],
   "source": [
    "all_y=0\n",
    "corr_y=0\n",
    "model.eval()\n",
    "model.cuda()\n",
    "half_mode=True\n",
    "data_loader = torch.utils.data.DataLoader(evalds,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=False,num_workers=10)\n",
    "preds=[]\n",
    "file_names=[]\n",
    "with torch.no_grad():\n",
    "    for i,(x,idx,y) in enumerate(data_loader):\n",
    "        if i%10==0:\n",
    "            print(i,\"/\",len(data_loader),end=\"\\r\")\n",
    "        x=x.cuda()\n",
    "        if half_mode:\n",
    "            x=x.half()\n",
    "        out=model(x)\n",
    "        all_y+=1\n",
    "        file_names+=[a for a in idx]\n",
    "        preds+=[out.cpu()]\n",
    "print(\"all \", all_y,\" correct \",corr_y,\" ratio=\",corr_y*1./all_y) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfiles,pprobs=torch.load(model_path+\"eval_predictions_probs.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfiles==file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching tensor(8640) out of  8640\n"
     ]
    }
   ],
   "source": [
    "print(\"matching\",(torch.argmax(pprobs.float(),1)== torch.argmax(torch.cat(preds,0).float(),1) ).sum(),\"out of \" ,pprobs.shape[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
